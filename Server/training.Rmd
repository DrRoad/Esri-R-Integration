---
title: "Model Training"
output:
  html_document:
    fig_caption: yes
    toc: yes
    toc_collapse: no
    toc_float: yes
---

The purpose of this notebook is to train a toy model to demonstrate how R modeling can be tied in with web-based frontends. In this case, we will use a shapefile that contains latitudes, longitudes, and a number of demography variables for polygons around the US. These include Median Income, so we will construct a toy model by attempting to predict Median Income as a function of the remaining variables. The model we'll be fitting is a simple [linear regression](http://r-statistics.co/Linear-Regression.html), which may or may not have predictive power in this case, but is simple to set up and use.

# Setup

## Libraries
The code below will attempt to require the given list of libraries, and automatically install any that are missing.
```{r warning==FALSE}
set.seed(1)
sapply(c(
	'plumber',
	'tidyverse',
	'caret',
	'rgdal',
	'rgeos'
), function(p) {
	if (!requireNamespace(p, quietly = TRUE)) {
		install.packages(p, quiet = TRUE)
	}
	require(p, character.only = TRUE, quietly = TRUE)
})
```

## Reading in a Shapefile

For the purposes of this example, we'll use the '2016 Population Density by Congressional District' shapefile provided by the Esri Demographics team, which can be found at [https://www.arcgis.com/home/item.html?id=ff48bbae433442a38f6c635b8c7baf72](https://www.arcgis.com/home/item.html?id=ff48bbae433442a38f6c635b8c7baf72). 

We've downloaded and unzipped it into the 'files' subdirectory, so we can now read it in using functions from R's 'RGDAL' library.
```{r}
shapefile <- rgdal::readOGR(
	dsn = "files/2016_Population_Density_by_Congressional_District.shp",
	stringsAsFactors = FALSE
)

shapefile@data <- shapefile@data %>%
	mutate_at(
	  vars(TOTPOP_CY:GenSilent), as.numeric
	)

stagedShapefile <- SpatialPointsDataFrame(gCentroid(shapefile, byid=TRUE), shapefile@data) %>%
  as.tibble() %>%
  select( colnames(.) %>% order ) %>%
  select( -OBJECTID, -ID, -NAME, -ST_ABBREV ) %>%
  select( x, y, everything() )

head(stagedShapefile)
```

Note that the shapefile data actually has Median Income data attached to it, so we can look at some summary statistics to get an idea of how it is actually distributed:
```{r fig.height = 6, fig.width = 20, fig.align = "center"}
summary(stagedShapefile$MEDHINC_CY)
boxplot(stagedShapefile$MEDHINC_CY, horizontal = TRUE)
hist(stagedShapefile$MEDHINC_CY, breaks = 250, freq = FALSE)
```

# Modeling

## Training

Before training the model, we do some slight preprocessing.

The first preprocessing step we'll use is to remove columns that are linearly dependent - that is, columns that can be obtained by adding or subtracting multiples of other columns, making them redundant.
```{r}
dfClean <- stagedShapefile[, -caret::findLinearCombos(stagedShapefile)$remove]
print(GetoptLong::qq("Linearly dependent columns removed: @{ncol(stagedShapefile) - ncol(dfClean)}"))
```

From this cleaned data set, we'll create some derived variables from the remaining columns, while dropping the columns they originally came from in order. This allows us to simulate categorical variables, and is also a technique that can be used when the exact numerical value of a regressor isn't terribly important but one still cares about its rough placement within the original distribution.
```{r}
dfClean2 <- dfClean %>%
  # Binning population density into 5 categories
  mutate(
    PopulationDensity = cut(
      POPDENS_CY, 
      breaks = quantile( .$POPDENS_CY, 0:5 * (1/5) ), 
      labels = c( "low", "medium-low", "medium", "medium-high", "high" )
    )
  ) %>%
  # Binning percentage of baby boomers into 3 categories
  mutate(
    PropBoomers = GenBoom / stagedShapefile$TOTPOP_CY
  ) %>%
  mutate(
    PropBoomers = cut(
      PropBoomers, 
      breaks = quantile( .$PropBoomers, 0:3 * (1/3) ), 
      labels = c( "low", "average", "high" )
    )
  ) %>%
  # Setting 3 flags for when demography variables are above average
  mutate(
    HighlyEducated = ifelse(GRADDEG_CY >= mean(stagedShapefile$GRADDEG_CY), TRUE, FALSE),
    ManyWidows = ifelse(WIDOWED_CY >= mean(stagedShapefile$WIDOWED_CY), TRUE, FALSE),
    LargePopulation = ifelse(HHPOP_CY >= mean(stagedShapefile$HHPOP_CY), TRUE, FALSE)
  ) %>% 
  # Dropping original columns
  select(-POPDENS_CY, -GenBoom, -GRADDEG_CY, -WIDOWED_CY, -HHPOP_CY) %>%
  # Rearrange columns to see relevant variables in output
  select( 
    x, y, MEDHINC_CY, PopulationDensity, PropBoomers, HighlyEducated, ManyWidows, LargePopulation, everything()
  ) %>%
  # Using quantiles may introduce 1 NA per variable at the 0% quantile, to simplify we just drop this observation
  drop_na

head(dfClean2)
```


Next, we'll split the full dataset into a training set (90% of the data) and a holdout or testing set (the remaining 10%). This will allow us to assess the model's accuracy by attempting to make predictions on the testing set, for which we know the actual response variable and can measure the difference.
```{r}
trainIndices <- createDataPartition(
  dfClean2 %>% pull(MEDHINC_CY),
  p = 0.90,
  list = FALSE,
  times = 1
)

training <- dfClean2[ trainIndices,]
test  <- dfClean2[-trainIndices,]
```

We'll now fit a linear model to the training set. Here, we're using the train function from caret, as it allows us to specify more preprocessing to be applied to the training data as well as future predictions. 

The first argument is a formula, and in this case we're specifying Median Income as a function of all remaining variables.

For preprocessing, we are centering and scaling the data so it has mean zero and standard deviation one. We also remove columns with near-zero variance (nzv), which tend to have be nearly constant across all of the training data, and then we apply a Yeo-Johnson transformation to stabilize the variance of the training data by making it more closely resemble a normal distribution.

After training the linear model, we can examine the coefficients of each variable in the regression.

```{r}
lmFit <- caret::train(
  MEDHINC_CY ~ . , 
  data = training, 
  method = "lm"
)
print(lmFit$finalModel$coefficients)
```

We now have a model that takes in a latitude, longitude, several derived variables, and a host of various demography variables, and attempts to predict the Median Income for that location from this information.

## Performance / Error

We can now measure the error in two ways - the training error and the test error.

The training error indicates how well the model predicts data it has already seen - or in other words, how well the model fits the training data. The testing error indicates how well the model might predict on new, unseen data.
```{r}
printStats <- function(y_true, y_pred) {
  print(GetoptLong::qq("MAE: @{MLmetrics::MAE(y_pred = y_pred, y_true = y_true) %>% round(4)}"))
  print(GetoptLong::qq("MAPE: @{MLmetrics::MAPE(y_pred = y_pred, y_true = y_true) %>% round(4)}"))
  print(GetoptLong::qq("RMSE: @{MLmetrics::RMSE(y_pred = y_pred, y_true = y_true) %>% round(4)}"))
  print(GetoptLong::qq("R^2: @{MLmetrics::R2_Score(y_pred = y_pred, y_true = y_true) %>% round(4)}"))
  print(GetoptLong::qq("Correlation: @{cor(y_pred, y_true)}"))
}
```

The error metrics used are:

* MAE: [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error)
* MAPE: [Mean Absolute Percent Error](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)
* $R^2$: [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)
* [Correlation](https://en.wikipedia.org/wiki/Correlation_and_dependence#Pearson's_product-moment_coefficient)

### Training Error

This asks the question: how well does this model perform on data it has already seen? Here we expect error to be low but not perfect, as zero error would indicate [overfitting](https://en.wikipedia.org/wiki/Overfitting), leading to high variance and unrealistic predictions.
```{r}
printStats(
  y_pred = predict(lmFit, training %>% select(-MEDHINC_CY)),
  y_true = training %>% pull(MEDHINC_CY)
)
```

### Testing Error
Now we ask the question: how well will this model perform on new, unseen data? Here we hope for error to be as low as possible.
```{r}
printStats(
  y_pred = predict(lmFit, test %>% select(-MEDHINC_CY)),
  y_true = test %>% pull(MEDHINC_CY)
)
```



## Saving

With a model in hand, can now save it out to file, which we can load at any time to stage new data and run predictions.
```{r}
readr::write_rds(lmFit, "files/linear_model.rds")
print('Model written to file')
```