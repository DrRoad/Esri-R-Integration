---
title: "Standalone Test"
output:
  html_document:
    fig_caption: yes
    toc: yes
    toc_collapse: no
    toc_float: yes
---

The purpose of this notebook is to indicate how data is transferred back and forth between the web-based frontend and the R backend. This example works 'offline' and can be used without bringing HTTP into the picture, and so this helps demonstrate a workflow of building and fine-tuning an API endpoint before actually attempting to deploy it. Once one is able to get their model working locally, productionalizing the code is then just a matter of modifying the code slightly to accomodate plumber's interface.

# Setup

Setup usually involves pulling in required libraries and static files that will be needed to run the model (see esri_demo.R).

## Libraries
First, we require the following libraries (which will be installed if they are not already) and set up a few functions.
```{r warning=FALSE}
options(digits = 22)

sapply(c(
  'plumber',
  'GetoptLong',
  'tidyverse',
  'rgeos',
  'rgdal',
  'measurements',
  'caret',
  'jsonlite'
), function(p) {
  if (!requireNamespace(p, quietly = TRUE)) {
    install.packages(p, quiet = TRUE)
  }
  require(p, character.only = TRUE, quietly = TRUE)
})
print("Libraries loaded. Now loading static files.")

strInterpolate <- GetoptLong::qq

numConv <- function(x) {
  return(x %>% as.character %>% as.numeric)
}

getSpatialData <- function(lat, long) {
  thisPoint <- SpatialPoints(
    coords = tibble(
      long = long,
      lat = lat
    ),
    proj4string = CRS(proj4string(shapefile))
  )
  result <- over(thisPoint, shapefile)
  return(result)
}

getSpatialVariable <- function(lat, long, variable) {
  result <- getSpatialData(lat, long)
  outputVar <- result[[variable]]
  return( if (outputVar %>% is.na) 0 else numConv(outputVar) )
}
```

## Static Files

Next, we pull in static files. In our case, it will be one shapefile, as well as the model saved from the training notebook.
```{r}
shapefile <- rgdal::readOGR(
  dsn = "files/2016_Population_Density_by_Congressional_District.shp",
  stringsAsFactors = FALSE
)

shapefile@data <- shapefile@data %>%
  mutate_at(vars(TOTPOP_CY:GenSilent), function(x) x %>% as.numeric %>% round(10))

plot(shapefile)

lmFit <- readr::read_rds('files/linear_model.rds')
```

# Frontend Mockup

Now we will simulate some new input coming from the frontend form
```{r}
formData <- tibble(
  Latitude = 32.7157,
  Longitude = -117.1611,
  LocationSquareFootage = 1000 # etc
)

inputDataframe <- jsonlite::toJSON(formData, auto_unbox = TRUE)

print(formData)
print(inputDataframe)
```

# Backend Mockup

Everything from this point on will be going through what should happen on the server side (see API.R).

## Parse Input

Data from the frontend form will be sent via JSON, so first we will parse the incoming data and convert it to something R can work with, such as a data frame (or in this case a tibble).
```{r}
input <- jsonlite::fromJSON(inputDataframe) %>%
  as.tibble %>%
  mutate(
    LocationSquareFootage = LocationSquareFootage %>% as.numeric,
    Latitude = Latitude %>% as.numeric,
    Longitude = Longitude %>% as.numeric
  )
print(input)
```

## Stage Spatial Data

We'll now 'stage' the data, i.e. format it to be passed into the model. In this instance, this will involve enriching the incoming point with demography from the shapefile and creating a data frame out of it.

```{r}
stageData <- getSpatialData(input$Latitude, input$Longitude) %>%
  as.tibble %>%
  select( -OBJECTID, -ID, -NAME, -ST_ABBREV) %>%
  mutate(
    x = input$Latitude,
    y = input$Longitude
  ) %>%
  select( x, y, everything() %>% order )

print(stageData)
```

## Run Model on Staged Data

We imported our saved model `lmFit` from and rds file earlier, so now we'll just feed it into the predict function along the staged data frame.

(Note that in this example, the shapefile actually does have median income data attached to it - for demonstration purposes, we're dropping this column so we can treat it as an unknown for the toy model to predict.)

```{r}
actual_medIncome <- stageData %>% pull(MEDHINC_CY) %>% round

predicted_medIncome <- predict(
  lmFit,
  stageData %>% select(-MEDHINC_CY)
) %>% as.numeric %>% round
```

- Actual Median Income: `r actual_medIncome`
- Predicted Median Income: `r predicted_medIncome`

## Format Output

Now that we have a prediction, we simply need to package everything up and send it back to the frontend. Although plumber takes care of this behind the scenes, the output will be automatically converted to JSON and it is instructive to see an instance of what this conversion looks like.

```{r}
outputObject = list(
  'Square Meters' = measurements::conv_unit(input$LocationSquareFootage, 'ft2', 'm2'),
  'Actual Median Income' = actual_medIncome %>% round,
  'Predicted Median Income' = predicted_medIncome %>% round,
  'Percent Error' = (100 * abs(actual_medIncome - predicted_medIncome) / actual_medIncome) %>% round(2)
)
```

Here's what the actual output object looks like in R:
```{r}
print( outputObject )
```

And here's what the JSON object that is returned to the frontend:
```{r}
print( jsonlite::toJSON(outputObject, auto_unbox = TRUE) )
```

And that's it - this object can now be parsed in Javascript and displayed in a variety of ways on the frontend.