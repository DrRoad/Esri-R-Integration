---
title: "Standalone Test"
output:
  html_document:
    fig_caption: yes
    toc: yes
    toc_collapse: no
    toc_float: yes
---

The purpose of this notebook is to indicate how data is transferred back and forth between the web-based frontend and the R backend. This example works 'offline' and can be used without bringing communications over HTTP into the picture, and so this helps demonstrate a workflow of building and fine-tuning an API endpoint before actually deploying it. 

Deployment heavily relies on the [R plumber package](https://www.rplumber.io/). Once one is able to get their model working locally, productionalizing the code is then just a matter of making several modifications to accomodate plumber's interface.

# Setup

Setup usually involves pulling in required libraries and static files that will be needed to run the model (see Server/esri_demo.R).

## Libraries
First, we require the following libraries (which will be installed if they are not already) and set up a few functions.
```{r warning=FALSE}
options(digits = 22)

sapply(c(
  'plumber',
  'GetoptLong',
  'tidyverse',
  'rgeos',
  'rgdal',
  'measurements',
  'caret',
  'jsonlite',
  'geosphere'
), function(p) {
  if (!requireNamespace(p, quietly = TRUE)) {
    install.packages(p, quiet = TRUE)
  }
  require(p, character.only = TRUE, quietly = TRUE)
})
print("Libraries loaded. Now loading static files.")

strInterpolate <- GetoptLong::qq

numConv <- function(x) {
  return(x %>% as.character %>% as.numeric)
}

getSpatialData <- function(lat, long) {
  thisPoint <- SpatialPoints(
    coords = tibble(
      long = long,
      lat = lat
    ),
    proj4string = CRS(proj4string(shapefile))
  )
  result <- over(thisPoint, shapefile)
  return(result)
}

getSpatialVariable <- function(lat, long, variable) {
  result <- getSpatialData(lat, long)
  outputVar <- result[[variable]]
  return( if (outputVar %>% is.na) 0 else numConv(outputVar) )
}
```

## Static Files

Next, we pull in static files. In our case, it will be the model file exported from the training notebook, as well as the saved shapefile.
```{r}
lmFit <- readr::read_rds('files/linear_model.rds')

shapefile <- rgdal::readOGR(
  dsn = "files/2016_Population_Density_by_Congressional_District.shp",
  stringsAsFactors = FALSE
)

shapefile@data <- shapefile@data %>%
  mutate_at(
    vars(TOTPOP_CY:GenSilent), 
    function(x) x %>% as.numeric %>% round(10)
  )

plot(shapefile)
```

Now we'll stage some of the shapefile data, just as we did when training the model.
```{r}
stagedDemographyData <- SpatialPointsDataFrame(gCentroid(shapefile, byid = TRUE), shapefile@data) %>%
  as.tibble() %>%
  select( colnames(.) %>% order ) %>%
  select( -OBJECTID, -ID, -NAME, -ST_ABBREV ) %>%
  select( x, y, everything() ) %>%
  # Dropping derived columns
  select(
    -POPDENS_CY, -GenBoom, -GRADDEG_CY, -WIDOWED_CY, -HHPOP_CY
  ) %>%
  # Rearrange columns to see relevant variables in output
  select( 
    x, y, everything()
  )

head(stagedDemographyData)
```

And similarly add in the simulated revenue data:
```{r fig.height = 6, fig.width = 20, fig.align = "center"}
set.seed(123)
stagedDemographyData$Revenue <- rnorm(n = nrow(shapefile), mean = 100000, sd = 10000)

summary( stagedDemographyData$Revenue )

ggplot( stagedDemographyData ) +
  geom_boxplot(
    aes(y = Revenue), notch = TRUE,
    fill = 'blue', alpha = 0.3,
    outlier.colour = "#1F3552", outlier.shape = 20
  ) +
  coord_flip() +
  theme_grey()

ggplot( stagedDemographyData, aes(x = Revenue) ) +
  geom_histogram( aes(y=..density..), bins = 100, fill = 'blue', alpha = 0.2 ) +
  geom_density( aes(y=..density..), fill = 'green', alpha = 0.2 )
```


# Frontend Mockup

We now simulate some new input that might come from the frontend form. For the purposes of this demonstration, we will have the user supply the 'derived' variables we set up during training, as well as the latitude/longitude for the new location - we will used these to run the model and return a revenue estimate:

```{r}
formData <- tibble(
  Latitude = 32.7157,
  Longitude = -117.1611,
  LocationSquareFootage = 1000,
  PopulationDensity = 'medium',
  PropBoomers = 'high',
  HighlyEducated = TRUE,
  ManyWidows = FALSE,
  LargePopulation = TRUE,
  NeighborsToUse = 5
)

inputDataframe <- jsonlite::toJSON(formData, auto_unbox = TRUE)

print(formData)
print(inputDataframe)
```

# Backend Mockup

Everything from this point on will be going through what should happen on the server side (see Server/api.R).

## Parse Input

Data from the frontend form will be sent via JSON, so first we will parse the incoming data and convert it to something R can work with such as a data frame (or in this case a tibble). 

In order to minimize errors, at this stage we'll also cast every variable to the appropriate type (string, integer, boolean, etc).
```{r}
input <- jsonlite::fromJSON(inputDataframe) %>%
  as.tibble %>%
  mutate(
    LocationSquareFootage = LocationSquareFootage %>% as.numeric,
    Latitude = Latitude %>% as.numeric,
    Longitude = Longitude %>% as.numeric,
    PopulationDensity = PopulationDensity %>% as.character,
    PropBoomers = PropBoomers %>% as.character,
    HighlyEducated = HighlyEducated %>% as.logical,
    ManyWidows = ManyWidows  %>% as.logical,
    LargePopulation = LargePopulation %>% as.logical,
    NeighborsToUse = NeighborsToUse %>% as.integer
  )
print(input)
```

## Stage Spatial Data

We'll now 'stage' the data, i.e. format it to be passed into the model. 

In production setups, this might involve querying the ArcGIS API or another data source to enrich the input point (for example by pulling demography variables). 

To simulate this step, we will compute the distance between the new point and all of the existing points in our shapefile, then take a weighted average of the $k$ nearest neighbors in order to supply the remaining demography variables. 

The parameter $k$ will be user-controlled, so the model can be fine-tuned from the frontend.

```{r}
# Compute the distance between this location and all other points in the shapefile
distances <- geosphere::distm(
    c(input$Longitude, input$Latitude), 
    stagedDemographyData[, 0:2] 
  ) %>% 
  t %>% 
  as_tibble %>% 
  mutate( index = 1:nrow(.) ) %>% 
  rename(Distance = V1)

# Pull the k nearest neighbors
neighboringDemography <- stagedDemographyData %>%
  mutate(
    Distance = distances$Distance
  ) %>%
  arrange(Distance) %>%
  head(input$NeighborsToUse) %>%
  mutate(
    InvDistance = (1/Distance) / sum(1/.$Distance)
  ) %>% 
  mutate(
    Contribution = InvDistance / sum(.$InvDistance)
  ) %>%
  select(
    x, y, Distance, Contribution, everything()
  )

print(neighboringDemography)
```

Given the $k$ nearest neighbors, we now just take a weighted sum of the existing demography from the shapefile, weighted inversely by distance, to obtain some simulated demography values for this new location:
```{r}
locationDemography <- neighboringDemography %>% 
  mutate_each( 
    funs( .*Contribution ), 
    ASSCDEG_CY:VACANT_FY
  ) %>% 
  mutate( collapseID = 1 ) %>%
  group_by( collapseID ) %>%
  summarize_all( funs(sum) ) %>% 
  select( ASSCDEG_CY:VACANT_FY )

print(locationDemography)
```

And now we adjoin all of this data to the user-supplied input:
```{r}
stagedData <- input %>%
  rename(
    x = Longitude,
    y = Latitude
  ) %>%
  mutate(joinID = 1) %>% 
  full_join(
    locationDemography %>% mutate(joinID = 1), 
    by = "joinID"
  )

head(stagedData)
```

## Run Model on Staged Data

We imported our saved model `lmFit` from a saved RDS file previously, so now we'll just feed it into the predict function along with data that we've just staged.

```{r}
predicted_revenue <- predict(
  lmFit,
  stagedData 
) %>% as.numeric
```

This yields a predicted revenue of $`r predicted_revenue %>% round(2)`

## Format Output

Now that we have a prediction, we simply need to package everything up and send it back to the frontend. Although plumber takes care of this behind the scenes by automatically converting all objects to JSON, it is instructive to see an instance of what this conversion looks like:

```{r}
outputObject = list(
  'Square Meters' = measurements::conv_unit(input$LocationSquareFootage, 'ft2', 'm2'),
  'Predicted Revenue' = predicted_revenue
)
```

Here's what the actual output object looks like in R:
```{r}
print( outputObject )
```

And here's what the JSON object that is returned to the frontend will look like:
```{r}
print( jsonlite::toJSON(outputObject, auto_unbox = TRUE) )
```

And that's it - this object can now be parsed in Javascript and displayed in a variety of ways on the frontend! See the `esri_demo` and `api` R scripts, particularly the `runmodel` endpoint, to see how code that runs locally in a notebook such as this can be adapted to work within an API endpoint.