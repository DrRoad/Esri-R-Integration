---
title: "Standalone Test"
output:
  html_document:
    fig_caption: yes
    toc: yes
    toc_collapse: no
    toc_float: yes
---

The purpose of this notebook is to indicate how data is transferred back and forth between the web-based frontend and the R backend. This example works 'offline' and can be used without bringing HTTP into the picture, and so this helps demonstrate a workflow of building and fine-tuning an API endpoint before actually attempting to deploy it. Once one is able to get their model working locally, productionalizing the code is then just a matter of modifying the code slightly to accomodate plumber's interface.

# Setup

Setup usually involves pulling in required libraries and static files that will be needed to run the model (see esri_demo.R).

## Libraries
First, we require the following libraries (which will be installed if they are not already) and set up a few functions.
```{r warning=FALSE}
options(digits = 22)

sapply(c(
  'plumber',
  'GetoptLong',
  'tidyverse',
  'rgeos',
  'rgdal',
  'measurements',
  'caret',
  'jsonlite',
  'geosphere'
), function(p) {
  if (!requireNamespace(p, quietly = TRUE)) {
    install.packages(p, quiet = TRUE)
  }
  require(p, character.only = TRUE, quietly = TRUE)
})
print("Libraries loaded. Now loading static files.")

strInterpolate <- GetoptLong::qq

numConv <- function(x) {
  return(x %>% as.character %>% as.numeric)
}

getSpatialData <- function(lat, long) {
  thisPoint <- SpatialPoints(
    coords = tibble(
      long = long,
      lat = lat
    ),
    proj4string = CRS(proj4string(shapefile))
  )
  result <- over(thisPoint, shapefile)
  return(result)
}

getSpatialVariable <- function(lat, long, variable) {
  result <- getSpatialData(lat, long)
  outputVar <- result[[variable]]
  return( if (outputVar %>% is.na) 0 else numConv(outputVar) )
}
```

## Static Files

Next, we pull in static files. In our case, it will be the model saved out of the previous training notebook, as well as the saved shapefile.
```{r}
lmFit <- readr::read_rds('files/linear_model.rds')

shapefile <- rgdal::readOGR(
  dsn = "files/2016_Population_Density_by_Congressional_District.shp",
  stringsAsFactors = FALSE
)

shapefile@data <- shapefile@data %>%
  mutate_at(
    vars(TOTPOP_CY:GenSilent), 
    function(x) x %>% as.numeric %>% round(10)
  ) 

plot(shapefile)
```

Now we'll stage some of the shapefile data, just as we did when training the model.
```{r}
stagedDemographyData <- SpatialPointsDataFrame(gCentroid(shapefile, byid = TRUE), shapefile@data) %>%
  as.tibble() %>%
  select( colnames(.) %>% order ) %>%
  select( -OBJECTID, -ID, -NAME, -ST_ABBREV ) %>%
  select( x, y, everything() ) %>%
  # Dropping derived columns
  select(
    -POPDENS_CY, -GenBoom, -GRADDEG_CY, -WIDOWED_CY, -HHPOP_CY
  ) %>%
  # Rearrange columns to see relevant variables in output
  select( 
    x, y, MEDHINC_CY, everything()
  )

head(stagedDemographyData)
```


# Frontend Mockup

Now we will simulate some new input coming from the frontend form. For the purposes of this demonstration, we will have the user supply the 'derived' variables we set up during training, as well as the latitude/longitude for the new location they want to predict off of:

```{r}
formData <- tibble(
  Latitude = 32.7157,
  Longitude = -117.1611,
  LocationSquareFootage = 1000,
  PopulationDensity = 'medium',
  PropBoomers = 'high',
  HighlyEducated = TRUE,
  ManyWidows = FALSE,
  LargePopulation = TRUE,
  NeighborsToUse = 5
)

inputDataframe <- jsonlite::toJSON(formData, auto_unbox = TRUE)

print(formData)
print(inputDataframe)
```

# Backend Mockup

Everything from this point on will be going through what should happen on the server side (see API.R).

## Parse Input

Data from the frontend form will be sent via JSON, so first we will parse the incoming data and convert it to something R can work with, such as a data frame (or in this case a tibble). In order to minimize errors, we'll also cast every variable to the appropriate type here (string, integer, boolean, etc).
```{r}
input <- jsonlite::fromJSON(inputDataframe) %>%
  as.tibble %>%
  mutate(
    LocationSquareFootage = LocationSquareFootage %>% as.numeric,
    Latitude = Latitude %>% as.numeric,
    Longitude = Longitude %>% as.numeric,
    PopulationDensity = PopulationDensity %>% as.character,
    PropBoomers = PropBoomers %>% as.character,
    HighlyEducated = HighlyEducated %>% as.logical,
    ManyWidows = ManyWidows  %>% as.logical,
    LargePopulation = LargePopulation %>% as.logical,
    NeighborsToUse = NeighborsToUse %>% as.integer
  )
print(input)
```

## Stage Spatial Data

We'll now 'stage' the data, i.e. format it to be passed into the model. 

In production setups, this might involve querying the ArcGIS API or another data source to enrich the input point, for example by pulling specific demography variables. To simulate this step, we will compute the distance between the new point and all of the existing points in our shapefile, then take a weighted average of the $k$ nearest neighbors in order to supply the remaining demography variables. (The parameter $k$ will be user-controlled, so the model can be fine-tuned from the frontend.)

```{r}
# Compute the distance between this location and all other points in the shapefile
distances <- geosphere::distm(
    c(input$Longitude, input$Latitude), 
    stagedDemographyData[, 0:2] 
  ) %>% 
  t %>% 
  as_tibble %>% 
  mutate( index = 1:nrow(.) ) %>% 
  rename(Distance = V1)

# Pull the k nearest neighbors
neighboringDemography <- stagedDemographyData %>%
  select(-MEDHINC_CY) %>%
  mutate(
    Distance = distances$Distance
  ) %>%
  arrange(Distance) %>%
  head(input$NeighborsToUse) %>%
  mutate(
    InvDistance = (1/Distance) / sum(1/.$Distance)
  ) %>% 
  mutate(
    Contribution = InvDistance / sum(.$InvDistance)
  ) %>%
  select(
    x, y, Distance, Contribution, everything()
  )

print(neighboringDemography)
```

Given the $k$ nearest neighbors, we now just takes a weight sum of the existing demography, weighted inversely by distance, to obtain simulated demography for the new location:
```{r}
locationDemography <- neighboringDemography %>% 
  mutate_each( 
    funs( .*Contribution ), 
    ASSCDEG_CY:VACANT_FY
  ) %>% 
  mutate( collapseID = 1 ) %>%
  group_by( collapseID ) %>%
  summarize_all( funs(sum) ) %>% 
  select( ASSCDEG_CY:VACANT_FY )

print(locationDemography)
```

And now we adjoin all of this data to the new input:
```{r}
stagedData <- input %>%
  rename(
    x = Longitude,
    y = Latitude
  ) %>%
  mutate(joinID = 1) %>% 
  full_join(
    locationDemography %>% mutate(joinID = 1), 
    by = "joinID"
  )

head(stagedData)
```

## Run Model on Staged Data

We imported our saved model `lmFit` from and rds file earlier, so now we'll just feed it into the predict function along the staged data frame.

(Note that in this example, the shapefile actually does have median income data attached to it - for demonstration purposes, we're dropping this column so we can treat it as an unknown for the toy model to predict.)

```{r}
actual_medIncome <- getSpatialVariable(input$Latitude, input$Longitude, 'MEDHINC_CY')

predicted_medIncome <- predict(
  lmFit,
  stagedData 
) %>% as.numeric %>% round
```

- Actual Median Income: `r actual_medIncome`
- Predicted Median Income: `r predicted_medIncome`

## Format Output

Now that we have a prediction, we simply need to package everything up and send it back to the frontend. Although plumber takes care of this behind the scenes, the output will be automatically converted to JSON and it is instructive to see an instance of what this conversion looks like.

```{r}
outputObject = list(
  'Square Meters' = measurements::conv_unit(input$LocationSquareFootage, 'ft2', 'm2'),
  'Actual Median Income' = actual_medIncome %>% round,
  'Predicted Median Income' = predicted_medIncome %>% round,
  'Percent Difference' = (100 * abs(actual_medIncome - predicted_medIncome) / actual_medIncome) %>% round(2)
)
```

Here's what the actual output object looks like in R:
```{r}
print( outputObject )
```

And here's what the JSON object that is returned to the frontend:
```{r}
print( jsonlite::toJSON(outputObject, auto_unbox = TRUE) )
```

And that's it - this object can now be parsed in Javascript and displayed in a variety of ways on the frontend.